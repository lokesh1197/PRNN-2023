{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09071dc4-5778-4cf5-84ec-e94e8a8bff4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcc2464-43fb-47e7-98d7-289b92809c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# for p-value out of significance test\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# for image data handling\n",
    "import os\n",
    "from os.path import join, isfile, dirname\n",
    "from PIL import Image\n",
    "\n",
    "# for svm\n",
    "from libsvm.svmutil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac9bdd-9a79-43b7-85c3-63a97dc2156c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b62ea-16b4-49a9-99e9-1ea5540e7c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Uncompress compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f0be7f-2f3c-420e-b02f-f58e5c9b2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -n ../data/images.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406ab32-c6bc-4307-9e80-c2cdb0432058",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81167db-99f0-4e02-8d35-4d9cc7fa9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFromImage(imageDir, size=(8, 8)):\n",
    "    dir = dirname(imageDir)\n",
    "    dataFile = join(dir, \"p4_data\") + str(size) + \".npy\"\n",
    "    if isfile(dataFile):\n",
    "        with open(dataFile, 'rb') as f:\n",
    "            return np.load(f)\n",
    "    \n",
    "    labels = os.listdir(imageDir)\n",
    "    image_data = [[] for _ in labels]\n",
    "    for label in labels:\n",
    "        dir = join(imageDir, label)\n",
    "        files = os.listdir(dir)\n",
    "        image_data[int(label)] = np.array([np.array(\n",
    "            Image.open(join(dir, file)).convert(\"L\").resize(size), dtype='uint8'\n",
    "        ) for file in files])\n",
    "        \n",
    "    image_data = np.array(image_data)\n",
    "    with open(dataFile, 'wb') as f:\n",
    "        np.save(f, image_data)\n",
    "    return image_data\n",
    "\n",
    "# returns X, Y, X_test, Y_test and classStats\n",
    "def trainTestSplit(data, train_ratio, func):\n",
    "    n = data.shape[0]\n",
    "    m = int(np.floor(data.shape[1] * train_ratio))\n",
    "    classStats = {}\n",
    "    x_train, y_train, x_test, y_test = [[[] for _ in range(n)] for _ in range(4)]\n",
    "    for label in range(n):\n",
    "        x_train[label], y_train[label], classStats[label] = func(label, data[label][:m], True)\n",
    "        x_test[label], y_test[label] = func(label, data[label][m:])\n",
    "    \n",
    "    X, Y, X_test, Y_test = [x.reshape(-1, x.shape[-1]) for x in [np.array(x) for x in [x_train, y_train, x_test, y_test]]]\n",
    "    return X, Y.flatten(), X_test, Y_test.flatten(), classStats\n",
    "\n",
    "def imgToFeatures(label, data, stats=False):\n",
    "    X = np.array([x.flatten() for x in data]) / 255\n",
    "    Y = label * np.ones(data.shape[0])\n",
    "    if stats:\n",
    "        return X, Y, { \"mean\": np.mean(X, axis=0), \"cov\": np.cov(X.T), \"prior\": data.shape[0], \"data\": X }\n",
    "    return X, Y\n",
    "\n",
    "def classify(x, classStats, density):\n",
    "    label = -1\n",
    "    max = -99999\n",
    "    sum = 0\n",
    "    prob = []\n",
    "    for key in classStats:\n",
    "        mean = classStats[key][\"mean\"]\n",
    "        cov = classStats[key][\"cov\"]\n",
    "        prior = classStats[key][\"prior\"]\n",
    "        weights = classStats[key][\"weights\"] if \"weights\" in classStats[key] else []\n",
    "        value = np.log(prior) + density(x, mean, cov, weights)\n",
    "        prob.append(value)\n",
    "        sum += value\n",
    "        if value > max:\n",
    "            max, label = value, key\n",
    "    return np.r_[[label], (np.array(prob) / sum)]\n",
    "\n",
    "class metrics:\n",
    "    def accuracy(predicted, actual):\n",
    "        m = actual.size\n",
    "        correctCount = sum([1 if int(predicted[i]) == int(actual[i]) else 0 for i in range(m)])\n",
    "        return correctCount / m\n",
    "    \n",
    "    def confusionMatrix(predicted, actual, n = 5):\n",
    "        cnf = np.zeros((n, n), dtype='uint')\n",
    "        for i in range(actual.size):\n",
    "            cnf[int(actual[i])][int(predicted[i])] += 1\n",
    "        return cnf\n",
    "    \n",
    "    def f1Score(cnf):\n",
    "        sum_predict = np.sum(cnf, axis=0)\n",
    "        sum_actual  = np.sum(cnf, axis=1)\n",
    "        f1 = np.zeros(cnf.shape[1])\n",
    "        for i in range(f1.size):\n",
    "            TP = cnf[i][i]\n",
    "            FP, FN = sum_predict[i] - TP, sum_actual[i] - TP\n",
    "            p, r = TP/(TP + FP + 1e-8), TP/(TP + FN + 1e-8)\n",
    "            f1[i] = 2 * p * r / (p + r + 1e-8)\n",
    "        return f1\n",
    "    \n",
    "    def print(X, Y, X_test, Y_test, classStats, density, result=True):\n",
    "        n_labels = len(classStats)\n",
    "        train = np.array([classify(x, classStats, density) for x in X])\n",
    "        test = np.array([classify(x, classStats, density) for x in X_test])\n",
    "        # train = classify(X, classStats, density)\n",
    "        # test = classify(X, classStats, density)\n",
    "        y_train, p_train = train.T[0], train.T[1:].T\n",
    "        y_test, p_test = test.T[0], test.T[1:].T\n",
    "\n",
    "        cnf_train = metrics.confusionMatrix(y_train, Y, n_labels)\n",
    "        cnf_test = metrics.confusionMatrix(y_test, Y_test, n_labels)\n",
    "        acc_train = metrics.accuracy(y_train, Y)\n",
    "        acc_test = metrics.accuracy(y_test, Y_test)\n",
    "        f1_train = metrics.f1Score(cnf_train)\n",
    "        f1_test = metrics.f1Score(cnf_test)\n",
    "\n",
    "        print(\"------------------ Train ---------------------\")\n",
    "        print(\"Classification Accuracy : \", acc_train)\n",
    "        print(\"F1 Score                : \", f1_train)\n",
    "        print(\"------------------ Test ----------------------\")\n",
    "        print(\"Classification Accuracy : \", acc_test)\n",
    "        print(\"F1 Score                : \", f1_test)\n",
    "\n",
    "        if result:\n",
    "            return [acc_train, f1_train], [acc_test, f1_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafbc18-d751-4f53-970a-db90960ec5df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547e5a6d-ddf9-437d-8861-4d7bf8d6eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Data Shapes ------------------------------\n",
      "    (Regression) p1[train]:       (10000, 3) , p1[test]:  (5000, 3)\n",
      "    (Regression) p2[train]:       (10000, 4) , p2[test]:  (5000, 4)\n",
      "(Classification) p3[train]:      (60000, 11) , p3[test]:  (15000, 11)\n",
      "(Classification)  p4[data]: (10, 6000, 16, 16)\n",
      "(Classification)  p5[data]:      (60000, 11)\n"
     ]
    }
   ],
   "source": [
    "dataFolder = \"../data\"\n",
    "imageDir = join(dataFolder, \"images\")\n",
    "imageDataDir = join(dataFolder, \"p4_data.csv\")\n",
    "\n",
    "p1 = { \"testDir\": dataFolder + \"/p1_test.csv\", \"trainDir\": dataFolder + \"/p1_train.csv\" } # regression\n",
    "p2 = { \"testDir\": dataFolder + \"/p2_test.csv\", \"trainDir\": dataFolder + \"/p2_train.csv\" } # regression\n",
    "p3 = { \"testDir\": dataFolder + \"/p3_test.csv\", \"trainDir\": dataFolder + \"/p3_train.csv\" } # classification\n",
    "p4 = {}                                                                                   # classification\n",
    "p5 = {}                                                                                   # classification\n",
    "\n",
    "p1[\"test\"] = np.genfromtxt(p1[\"testDir\"], delimiter=',')\n",
    "p1[\"train\"] = np.genfromtxt(p1[\"trainDir\"], delimiter=',')\n",
    "p2[\"test\"] = np.genfromtxt(p2[\"testDir\"], delimiter=',')\n",
    "p2[\"train\"] = np.genfromtxt(p2[\"trainDir\"], delimiter=',')\n",
    "p3[\"test\"] = np.genfromtxt(p3[\"testDir\"], delimiter=',')\n",
    "p3[\"train\"] = np.genfromtxt(p3[\"trainDir\"], delimiter=',')\n",
    "p4[\"data\"] = genFromImage(imageDir, (16, 16))\n",
    "p5[\"data\"] = np.genfromtxt(dataFolder + \"/PCA_MNIST.csv\", delimiter=',')[1:]\n",
    "\n",
    "print(\"--------------------------- Data Shapes ------------------------------\")\n",
    "print(\"    (Regression) p1[train]:      \", p1[\"train\"].shape, \", p1[test]: \", p1[\"test\"].shape)\n",
    "print(\"    (Regression) p2[train]:      \", p2[\"train\"].shape, \", p2[test]: \", p2[\"test\"].shape)\n",
    "print(\"(Classification) p3[train]:     \", p3[\"train\"].shape, \", p3[test]: \", p3[\"test\"].shape)\n",
    "print(\"(Classification)  p4[data]:\", p4[\"data\"].shape)\n",
    "print(\"(Classification)  p5[data]:     \", p5[\"data\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337fbae-68a6-4b90-a8ac-27c8063205a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# P4 (Neural Networks, MLP)\n",
    "\n",
    "- Construct a Multi-layer Perception (MLP) or a feed-forward neural network to work on the K-MNIST dataset. \n",
    "- Experiment with at least 3 settings of the number of hidden layers and Neurons. \n",
    "- Explicitly code the Error Backpropagation algorithm as a class and use it on MLPs with different architectures and loss functions (CE, squared error loss).\n",
    "- For this part, you should only use Numpy. \n",
    "\n",
    "Report the accuracy and F1 scores with all the considered configurations.\n",
    "\n",
    "**DATA:** `images.zip(p4[\"data\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cc0e8ed-efe3-416f-8a62-f926a27ffd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  2,  4,  6],\n",
       "        [ 8, 10, 12, 14],\n",
       "        [16, 18, 20, 22]],\n",
       "\n",
       "       [[24, 26, 28, 30],\n",
       "        [32, 34, 36, 38],\n",
       "        [40, 42, 44, 46]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(24).reshape(2,3,4)\n",
    "b = 2 * a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531fb2f8-c13c-4119-baf1-ad604fa5e198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0986122886681098, 1.3862943611198906]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3], [3,4,5]]\n",
    "b = [2, 1]\n",
    "# np.log(a[range(2), b])\n",
    "[np.log(a[i][b[i]]) for i in range(len(b))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772e62d-0afe-40a6-aef8-b96278d2732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers: array of number of neurons per layer including input and output\n",
    "def initialize(layers):\n",
    "    weights = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights.append(np.random.rand(layers[i] + 1, layers[i+1]))\n",
    "    return np.array(weights)\n",
    "\n",
    "def forward_propagation(X, weights, layers):\n",
    "    y, cache = X, [X]\n",
    "    for i, l in enumerate(layers):\n",
    "        y = np.c_[np.ones(len(y)), y] @ weights[i]\n",
    "        cache.append(y)\n",
    "        y = l[1](y)\n",
    "    return y, cache\n",
    "\n",
    "def backward_propagation(output, y, weights, layers, cache):\n",
    "    delta = lossGradient(output, y)\n",
    "    for i, l in reversed(list(enumerate(layers))):\n",
    "        delta = np.multiply(weights[i].T @ delta, grad(l[1])(cache[i])) # not cache[i -1] as len(cache) = 1 + len(layers)\n",
    "        gradients.append(cache[i] @ delta\n",
    "    return gradients\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "# Cross-entropy loss with softmax\n",
    "def computeLoss(output, y):\n",
    "    p = softmax(output)\n",
    "    return np.sum([-np.log(p[i][j]) for i, j in enumerate(y)]) / len(y)\n",
    "\n",
    "# Cross-entropy loss with softmax\n",
    "def lossGradient(output, y):\n",
    "    p = softmax(output)\n",
    "    return np.array([p[i][j] - 1 for i, j in enumerate(y)]) / len(y)\n",
    "\n",
    "def accuracy(p, y):\n",
    "    pred = np.argmax(p, axis=1)\n",
    "    return np.sum(np.double(np.abs(pred - y) < 0.5)) / len(y)\n",
    "\n",
    "# layers: array of tuples of number of neurons per layer and activation function excluding input\n",
    "# example: [(2, relu), (5, sigmoid)]\n",
    "# alpha: learning rate\n",
    "def train(X, Y, layers, epochs, alpha):\n",
    "    labels = np.unique(Y)\n",
    "    weights = initialize([X.shape[1], *[l[0] for l in layers]])\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        output, cache = forward_propagation(X, weights, layers)\n",
    "        \n",
    "        loss = computeLoss(output, Y)\n",
    "        loss_history.append(loss)\n",
    "        accuracy = accuracy(output, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        gradients = backward_propagation(output, Y, weights, layers, cache)\n",
    "        weights = weights - alpha * gradients\n",
    "        \n",
    "    return weights, loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e444d8-c4ec-4633-90ec-be138198f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3[\"train\"].shape, p3[\"test\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101591c-28fe-4081-a6fd-461d228485cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classStats = {}\n",
    "for row in p3[\"train\"]:\n",
    "    label = int(row[-1]) - 1\n",
    "    if label in classStats:\n",
    "        classStats[label].append(row[:-1])\n",
    "    else:\n",
    "        classStats[label] = [row[:-1]]\n",
    "\n",
    "# classStats = [np.array(data) for data in classStats]\n",
    "for i in range(len(classStats)):\n",
    "    data = np.array(classStats[i])\n",
    "    classStats[i] = { \"mean\": np.mean(data, axis=0), \"cov\": np.cov(data.T), \"prior\": data.shape[0], \"data\": data }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0374a04-65ff-42ae-9a49-14365afcb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(data):\n",
    "    # X = np.array([normalize(col) for col in data.T[:-1]]).T\n",
    "    X = data.T[:-1].T\n",
    "    Y = data.T[-1].T.astype(\"int\") - 1\n",
    "    return X, Y\n",
    "\n",
    "X, Y = splitData(p3[\"train\"])\n",
    "X_test, Y_test = splitData(p3[\"test\"])\n",
    "\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a0864-a6c0-4916-899d-66714309ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3[\"result\"] = [[] for _ in range(5)]\n",
    "p3[\"result\"][0] = metrics.print(X, Y, X_test, Y_test, classStats, logNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4ffda-da5a-4cff-8bb9-7814723417d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# P5 (Neural Networks, CNN)\n",
    "\n",
    "- Construct a CNN for the K-MNIST dataset and code the back-propagation algorithm with weight sharing and local-receptive fields. \n",
    "- Experiment with 3 different architectures and report the accuracy.\n",
    "\n",
    "**DATA:** `images.zip (p4[data])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76191a41-0109-4be9-9968-ecdd21e4f66e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f63a3-994d-43bb-901a-3c8aa17d0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"splitData\"] = [trainTestSplit(p4[\"data\"], r, imgToFeatures) for r in [0.2, 0.3, 0.5, 0.7, 0.9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d38672-be8d-40b2-b92e-e055e8d6257f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d372a03-c0d3-4133-94ae-7200cadb32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"] = [[] for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5ea59-0c62-4f83-865f-cb0eea1a97c9",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c82628-477d-401d-b21d-5838b95b6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"][0] = metrics.print(*p4[\"splitData\"][0], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370e7b4-288c-4aa3-a048-991ee693b29d",
   "metadata": {},
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42ff52-3adf-4c27-b98f-cb6cd1969ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"][0] = metrics.print(*p4[\"splitData\"][1], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdd059-82ff-4cf6-abf3-562fbfb2eb5c",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724daf75-59b0-4d69-af19-16f867d4d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"][0] = metrics.print(*p4[\"splitData\"][2], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e83e7c-7838-4036-9ce1-218ee0bd4686",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc577f00-f0ed-4fb8-8aab-85a39816f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"][0] = metrics.print(*p4[\"splitData\"][3], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ea73e-9460-4d11-877d-1b156e720fbb",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b642c4-0d96-4318-abb3-1e1eab4e47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"result\"][0] = metrics.print(*p4[\"splitData\"][4], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef07930-8c45-400a-a5b6-1fc6b9ef2c7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479603b-b37b-4794-a865-ce2379ce1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGmm(data, number_of_guassians=2):\n",
    "    classStatsGMM = {}\n",
    "    for label in data[-1]:\n",
    "        classStatsGMM[label] = { \"prior\": data[-1][label][\"prior\"] }\n",
    "        classStatsGMM[label][\"weights\"], classStatsGMM[label][\"mean\"], classStatsGMM[label][\"cov\"] = em(data[-1][label][\"data\"], number_of_guassians, 50)\n",
    "\n",
    "    metrics.print(*data[:-1], classStatsGMM, logGMM, result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00028ddd-684b-40c6-8a56-4b7ae6aa1d17",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19db03-3f67-4352-bb43-99571ca6e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p4[\"splitData\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac49534-ea7a-44a9-b40a-c731b219c858",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e3e3e-bc68-4895-a6f5-a05a9272ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p4[\"splitData\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104532fe-2c9e-4dce-9ddd-7e98cfb11ef9",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563d70b-0329-4b53-9581-bde21505e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p4[\"splitData\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ced0f-66a7-4d8e-ae30-0c70fef0a13e",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c16882-c799-42d2-8d3c-bc30c399a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p4[\"splitData\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc894a-a9e3-41bd-9671-52bcba69517b",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2258b6-7747-4419-8b57-0e31b1299397",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p4[\"splitData\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea00d9-ca3b-4b22-9d58-4f5591e9106a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6350980-b2a9-4902-8f3f-bd22e8ae8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegressor(data):\n",
    "    X_train,y_train_orig , X_test, y_test_orig, classStats = data\n",
    "    num_classes = 10\n",
    "    num_samples = y_train_orig.shape[0]\n",
    "    y_train = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        y_train[i, int(y_train_orig[i]) - 1] = 1\n",
    "\n",
    "    # Define sigmoid function\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Define softmax function\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    num_features = X_train.shape[1]\n",
    "    W = np.random.randn(num_features, num_classes)\n",
    "    b = np.random.randn(num_classes)\n",
    "\n",
    "    # Set hyperparameters\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 1000\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Train model using gradient descent\n",
    "    prev_loss = float('inf')\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        z = np.dot(X_train, W) + b\n",
    "        y_pred = softmax(z)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -np.sum(y_train * np.log(y_pred + epsilon)) / num_samples\n",
    "\n",
    "        # Backward propagation\n",
    "        dz = y_pred - y_train\n",
    "        dW = np.dot(X_train.T, dz) / num_samples\n",
    "        db = np.sum(dz, axis=0) / num_samples\n",
    "\n",
    "        # Update weights and biases\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        # Check stopping criterion\n",
    "        if prev_loss - loss < epsilon:\n",
    "            print('Stopping criterion met')\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    z = np.dot(X_test, W) + b\n",
    "    y_pred = np.argmax(softmax(z), axis=1) + 1\n",
    "    accuracy = np.sum(y_pred == y_test_orig) / y_test_orig.shape[0]\n",
    "    print('Test accuracy:', accuracy)\n",
    "\n",
    "    z_train = np.dot(X_train, W) + b\n",
    "    y_train_pred = np.argmax(softmax(z_train), axis=1) + 1\n",
    "    train_loss = -np.sum(y_train * np.log(softmax(z_train) + epsilon)) / num_samples\n",
    "    train_error_rate = 1 - np.sum(y_train_pred == y_train_orig) / y_train_orig.shape[0]\n",
    "    print('Training empirical risk:', train_loss)\n",
    "    print('Training error rate:', train_error_rate)\n",
    "\n",
    "    # Compute empirical risk on test data\n",
    "    num_samples_test = y_test_orig.shape[0]\n",
    "    y_test = np.zeros((num_samples_test, num_classes))\n",
    "    for i in range(num_samples_test):\n",
    "        y_test[i, int(y_test_orig[i]) - 1] = 1\n",
    "\n",
    "    z_test = np.dot(X_test, W) + b\n",
    "    test_loss = -np.sum(y_test * np.log(softmax(z_test) + epsilon)) / num_samples_test\n",
    "    test_error_rate = 1 - np.sum(y_pred == y_test_orig) / y_test_orig.shape[0]\n",
    "    print('Test empirical risk:', test_loss)\n",
    "    print('Test error rate:', test_error_rate)\n",
    "\n",
    "    num_classes = len(np.unique(y_test_orig))\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    for i in range(len(y_test_orig)):\n",
    "        true_class = int(y_test_orig[i] - 1)\n",
    "        predicted_class = int(y_pred[i] - 1)\n",
    "        confusion_matrix[true_class, predicted_class] += 1\n",
    "    # print('Confusion matrix:')\n",
    "    # print(confusion_matrix)\n",
    "\n",
    "\n",
    "    num_classes = len(np.unique(y_test_orig))\n",
    "    f1_scores = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        true_positives = confusion_matrix[i, i]\n",
    "        false_positives = np.sum(confusion_matrix[:, i]) - true_positives\n",
    "        false_negatives = np.sum(confusion_matrix[i, :]) - true_positives\n",
    "        precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "        recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "        f1_scores[i] = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    print('Class-wise F1 score:')\n",
    "    print(f1_scores)\n",
    "\n",
    "    # Choose two classes\n",
    "    class_1 = 1\n",
    "    class_2 = 2\n",
    "    # Get predicted probabilities for the two classes\n",
    "    y_class_1 = y_pred == class_1\n",
    "    y_class_2 = y_pred == class_2\n",
    "    y_prob_1 = softmax(z)[:, class_1 - 1]\n",
    "    y_prob_2 = softmax(z)[:, class_2 - 1]\n",
    "\n",
    "    # Compute true positive rate and false positive rate for both classes\n",
    "    num_thresholds = 100\n",
    "    tpr_class_1 = np.zeros(num_thresholds)\n",
    "    fpr_class_1 = np.zeros(num_thresholds)\n",
    "    tpr_class_2 = np.zeros(num_thresholds)\n",
    "    fpr_class_2 = np.zeros(num_thresholds)\n",
    "\n",
    "    for i in range(num_thresholds):\n",
    "        threshold = i / (num_thresholds - 1)\n",
    "        tp_class_1 = np.sum((y_prob_1 >= threshold) & (y_class_1 == True))\n",
    "        fn_class_1 = np.sum((y_prob_1 < threshold) & (y_class_1 == True))\n",
    "        tn_class_1 = np.sum((y_prob_2 < threshold) & (y_class_2 == True))\n",
    "        fp_class_1 = np.sum((y_prob_2 >= threshold) & (y_class_2 == False))\n",
    "        tpr_class_1[i] = tp_class_1 / (tp_class_1 + fn_class_1 + 1e-8)\n",
    "        fpr_class_1[i] = fp_class_1 / (fp_class_1 + tn_class_1 + 1e-8)\n",
    "\n",
    "        tp_class_2 = np.sum((y_prob_2 >= threshold) & (y_class_2 == True))\n",
    "        fn_class_2 = np.sum((y_prob_2 < threshold) & (y_class_2 == True))\n",
    "        tn_class_2 = np.sum((y_prob_1 < threshold) & (y_class_1 == True))\n",
    "        fp_class_2 = np.sum((y_prob_1 >= threshold) & (y_class_1 == False))\n",
    "        tpr_class_2[i] = tp_class_2 / (tp_class_2 + fn_class_2 + 1e-8)\n",
    "        fpr_class_2[i] = fp_class_2 / (fp_class_2 + tn_class_2 + 1e-8)\n",
    "\n",
    "    # Plot RoC curves and confusion matrix\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    ax[0, 0].matshow(confusion_matrix, cmap='GnBu')\n",
    "    ax[0, 0].set_xlabel(\"Predicted\")\n",
    "    ax[0, 0].set_ylabel(\"Actual\")\n",
    "    ax[0, 0].set_title(\"Confusion Matrix\")\n",
    "    for (x, y), value in np.ndenumerate(confusion_matrix):\n",
    "        ax[0, 0].text(x, y, f\"{value: .0f}\", va=\"center\", ha=\"center\")\n",
    "\n",
    "    ax[0, 1].plot(fpr_class_1, tpr_class_1, marker='x')\n",
    "    ax[0, 1].set_xlabel(\"False positive rate\")\n",
    "    ax[0, 1].set_ylabel(\"True positive rate\")                     \n",
    "    ax[0, 1].set_title(\"ROC curve for class {}\".format(class_1))\n",
    "\n",
    "    ax[1, 0].plot(fpr_class_2, tpr_class_2, marker='x')\n",
    "    ax[1, 0].set_xlabel(\"False positive rate\")\n",
    "    ax[1, 0].set_ylabel(\"True Positive rate\")\n",
    "    ax[1, 0].set_title(\"ROC curve for class {}\".format(class_2))\n",
    "\n",
    "    ax[1, 1].plot(fpr_class_1, tpr_class_1, marker='x', label=\"Class {}\".format(class_1))\n",
    "    ax[1, 1].plot(fpr_class_2, tpr_class_2, marker='o', label=\"Class {}\".format(class_2))\n",
    "    ax[1, 1].set_xlabel(\"False positive rate\")\n",
    "    ax[1, 1].set_ylabel(\"True positive rate\")\n",
    "    ax[1, 1].set_title(\"ROC curve for classes {} and {}\".format(class_1, class_2))\n",
    "    ax[1, 1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafd59b-f57a-43b4-ad17-8d0036d694f0",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fc528-e54f-481d-826f-338899535abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p4[\"splitData\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca33f6e-361f-4b4a-92a2-5ce6e7fc8f15",
   "metadata": {},
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328bf57-03ea-4c56-b682-37ade5289984",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p4[\"splitData\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c37905-43e7-48e9-80da-a4c9e9cc28f9",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025d6bf-cff3-4a9d-a760-02b526faf454",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p4[\"splitData\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1a2cb-4608-493d-9b47-3e1b0187f5ce",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c054f71-6936-4dde-b3bc-da2c976e9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p4[\"splitData\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f8ab7-5eec-45fb-b715-5eccae94a0e9",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4acf9-1e4e-4562-894c-e9a2b439fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p4[\"splitData\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55265079-c87a-4aa6-a23d-865a1ad9f853",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# P7 (Neural Networks, MLP)\n",
    "\n",
    "Train an MLP on the PCA counterpart of the KMINST dataset and report your observations\n",
    "\n",
    "**DATA:** `p5[data]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d81006-03a9-483b-9f8e-77a97ebefd74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee7d03-30c0-42fe-a0f4-d6fb82cad285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(label, data, stats=False):\n",
    "    X = data\n",
    "    Y = label * np.ones(data.shape[0])\n",
    "    if stats:\n",
    "        return X, Y, { \"mean\": np.mean(X, axis=0), \"cov\": np.cov(X.T), \"prior\": data.shape[0], \"data\": X }\n",
    "    return X, Y\n",
    "\n",
    "classWiseData = [[] for _ in range(10)]\n",
    "for row in p5[\"data\"]:\n",
    "    label = int(row[0])\n",
    "    classWiseData[label].append(row[1:])\n",
    "    \n",
    "p5[\"splitData\"] = [trainTestSplit(np.array(classWiseData), r, stats) for r in [0.2, 0.3, 0.5, 0.7, 0.9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4219de3-6f0c-4cb4-bf97-d344d268855f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32906cc7-94fc-4f02-b3bb-1ceb394e10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"] = [[] for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4d490-7fe3-411a-8164-25bb470a6182",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d0f5d-8931-480c-94ed-3d33c7e82ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"][0] = metrics.print(*p5[\"splitData\"][0], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580e7ac-53d3-47cb-be97-19ecee28ec37",
   "metadata": {},
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d17d9b-5661-4f8a-b8d8-214ef031bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"][0] = metrics.print(*p5[\"splitData\"][1], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c22e33-4612-4a7a-81c0-18ecc92f061b",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f4580-6d9e-4a82-9c27-4031ea3034f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"][0] = metrics.print(*p5[\"splitData\"][2], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5b2d9-f600-4455-8433-5abf36b479ab",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9132c46-511e-47a4-9187-1c42bdbb18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"][0] = metrics.print(*p5[\"splitData\"][3], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b607f4f-fcc8-4c5d-ad50-a64c61291aff",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46fc6d-3247-4f1c-b268-66597a6d14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"result\"][0] = metrics.print(*p5[\"splitData\"][4], naiveLogNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b1167-96d2-4453-8cfc-a1010eebb8ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16d37-5959-41a2-8440-4e4ced85e0b6",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b2955-ba0f-4549-9fce-52dfa092f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p5[\"splitData\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39229df0-161e-4f4a-a4ca-83ece32099a4",
   "metadata": {},
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5af97-3cce-4e77-9524-b0d1a58b8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p5[\"splitData\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4d182-9613-4285-beb7-2bb1de483bc7",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845131b-40c0-4c1f-b9e5-0738b8b8ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p5[\"splitData\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d08ac-d2c7-4871-8cf1-a8ba109a028e",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3d2d1-e756-47b5-b641-1530794530d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p5[\"splitData\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a5439-29e5-4d40-8b56-5fbb938d67b8",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c217c5-b631-468e-822d-ce9550af785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "printGmm(p5[\"splitData\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185808fa-3db4-4e62-9c61-98a6bf250bc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26880ac-4d22-4ef4-b48d-78c393bf2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegressor(data):\n",
    "    X_train,y_train_orig , X_test, y_test_orig, classStats = data\n",
    "    num_classes = 10\n",
    "    num_samples = y_train_orig.shape[0]\n",
    "    y_train = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        y_train[i, int(y_train_orig[i]) - 1] = 1\n",
    "\n",
    "    # Define sigmoid function\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Define softmax function\n",
    "    def softmax(x):\n",
    "        # subtract the maximum value from x to avoid overflow\n",
    "        x -= np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        # divide by the sum of the exponential values along axis 1\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True, where=np.isfinite(exp_x))\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    num_features = X_train.shape[1]\n",
    "    W = np.random.randn(num_features, num_classes)\n",
    "    b = np.random.randn(num_classes)\n",
    "\n",
    "    # Set hyperparameters\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 1000\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Train model using gradient descent\n",
    "    prev_loss = float('inf')\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        z = np.dot(X_train, W) + b\n",
    "        y_pred = softmax(z)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -np.sum(y_train * np.log(y_pred + epsilon)) / num_samples\n",
    "\n",
    "        # Backward propagation\n",
    "        dz = y_pred - y_train\n",
    "        dW = np.dot(X_train.T, dz) / num_samples\n",
    "        db = np.sum(dz, axis=0) / num_samples\n",
    "\n",
    "        # Update weights and biases\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        # Check stopping criterion\n",
    "        if prev_loss - loss < epsilon:\n",
    "            print('Stopping criterion met')\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    z = np.dot(X_test, W) + b\n",
    "    y_pred = np.argmax(softmax(z), axis=1) + 1\n",
    "    accuracy = np.sum(y_pred == y_test_orig) / y_test_orig.shape[0]\n",
    "    print('Test accuracy:', accuracy)\n",
    "\n",
    "    z_train = np.dot(X_train, W) + b\n",
    "    y_train_pred = np.argmax(softmax(z_train), axis=1) + 1\n",
    "    train_loss = -np.sum(y_train * np.log(softmax(z_train) + epsilon)) / num_samples\n",
    "    train_error_rate = 1 - np.sum(y_train_pred == y_train_orig) / y_train_orig.shape[0]\n",
    "    print('Training empirical risk:', train_loss)\n",
    "    print('Training error rate:', train_error_rate)\n",
    "\n",
    "    # Compute empirical risk on test data\n",
    "    num_samples_test = y_test_orig.shape[0]\n",
    "    y_test = np.zeros((num_samples_test, num_classes))\n",
    "    for i in range(num_samples_test):\n",
    "        y_test[i, int(y_test_orig[i]) - 1] = 1\n",
    "\n",
    "    z_test = np.dot(X_test, W) + b\n",
    "    test_loss = -np.sum(y_test * np.log(softmax(z_test) + epsilon)) / num_samples_test\n",
    "    test_error_rate = 1 - np.sum(y_pred == y_test_orig) / y_test_orig.shape[0]\n",
    "    print('Test empirical risk:', test_loss)\n",
    "    print('Test error rate:', test_error_rate)\n",
    "\n",
    "    num_classes = len(np.unique(y_test_orig))\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    for i in range(len(y_test_orig)):\n",
    "        true_class = int(y_test_orig[i] - 1)\n",
    "        predicted_class = int(y_pred[i] - 1)\n",
    "        confusion_matrix[true_class, predicted_class] += 1\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    num_classes = len(np.unique(y_test_orig))\n",
    "    f1_scores = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        true_positives = confusion_matrix[i, i]\n",
    "        false_positives = np.sum(confusion_matrix[:, i]) - true_positives\n",
    "        false_negatives = np.sum(confusion_matrix[i, :]) - true_positives\n",
    "        precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "        recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "        f1_scores[i] = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    print('Class-wise F1 score:')\n",
    "    print(f1_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # Choose two classes\n",
    "    class_1 = 1\n",
    "    class_2 = 2\n",
    "\n",
    "    # Get predicted probabilities for the two classes\n",
    "    y_class_1 = y_test_orig == class_1\n",
    "    y_class_2 = y_test_orig == class_2\n",
    "    y_prob_1 = softmax(z_test)[:, class_1 - 1]\n",
    "    y_prob_2 = softmax(z_test)[:, class_2 - 1]\n",
    "\n",
    "    # Compute true positive rate and false positive rate for both classes\n",
    "    num_thresholds = 100\n",
    "    tpr_class_1 = np.zeros(num_thresholds)\n",
    "    fpr_class_1 = np.zeros(num_thresholds)\n",
    "    tpr_class_2 = np.zeros(num_thresholds)\n",
    "    fpr_class_2 = np.zeros(num_thresholds)\n",
    "\n",
    "    for i in range(num_thresholds):\n",
    "        threshold = i / (num_thresholds - 1)\n",
    "        tp_class_1 = np.sum(y_class_1 & (y_prob_1 > threshold))\n",
    "        fp_class_1 = np.sum(~y_class_1 & (y_prob_1 > threshold))\n",
    "        tn_class_1 = np.sum(~y_class_1 & (y_prob_1 <= threshold))\n",
    "        fn_class_1 = np.sum(y_class_1 & (y_prob_1 <= threshold))\n",
    "        tpr_class_1[i] = tp_class_1 / (tp_class_1 + fn_class_1)\n",
    "        fpr_class_1[i] = fp_class_1 / (fp_class_1 + tn_class_1)\n",
    "\n",
    "        tp_class_2 = np.sum(y_class_2 & (y_prob_2 > threshold))\n",
    "        fp_class_2 = np.sum(~y_class_2 & (y_prob_2 > threshold))\n",
    "        tn_class_2 = np.sum(~y_class_2 & (y_prob_2 <= threshold))\n",
    "        fn_class_2 = np.sum(y_class_2 & (y_prob_2 <= threshold))\n",
    "        tpr_class_2[i] = tp_class_2 / (tp_class_2 + fn_class_2)\n",
    "        fpr_class_2[i] = fp_class_2 / (fp_class_2 + tn_class_2)\n",
    "        \n",
    "    \n",
    "    # Plot RoC curves and confusion matrix\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    ax[0, 0].matshow(confusion_matrix, cmap='GnBu')\n",
    "    ax[0, 0].set_xlabel(\"Predicted\")\n",
    "    ax[0, 0].set_ylabel(\"Actual\")\n",
    "    ax[0, 0].set_title(\"Confusion Matrix\")\n",
    "    for (x, y), value in np.ndenumerate(confusion_matrix):\n",
    "        ax[0, 0].text(x, y, f\"{value: .0f}\", va=\"center\", ha=\"center\")\n",
    "\n",
    "\n",
    "    ax[0, 1].plot(fpr_class_1, tpr_class_1, marker='x')\n",
    "    ax[0, 1].set_xlabel(\"False positive rate\")\n",
    "    ax[0, 1].set_ylabel(\"True positive rate\")                     \n",
    "    ax[0, 1].set_title(\"ROC curve for class {}\".format(class_1))\n",
    "\n",
    "    ax[1, 0].plot(fpr_class_2, tpr_class_2, marker='x')\n",
    "    ax[1, 0].set_xlabel(\"False positive rate\")\n",
    "    ax[1, 0].set_ylabel(\"True Positive rate\")\n",
    "    ax[1, 0].set_title(\"ROC curve for class {}\".format(class_2))\n",
    "\n",
    "    ax[1, 1].plot(fpr_class_1, tpr_class_1, marker='x', label=\"Class {}\".format(class_1))\n",
    "    ax[1, 1].plot(fpr_class_2, tpr_class_2, marker='o', label=\"Class {}\".format(class_2))\n",
    "    ax[1, 1].set_xlabel(\"False positive rate\")\n",
    "    ax[1, 1].set_ylabel(\"True positive rate\")\n",
    "    ax[1, 1].set_title(\"ROC curve for classes {} and {}\".format(class_1, class_2))\n",
    "    ax[1, 1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1ba6a-1e61-445b-9639-c0d03f50ec16",
   "metadata": {},
   "source": [
    "### Test split -- 20:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d64636-8e2e-4237-8d77-10043b08e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p5[\"splitData\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff800c66-70c9-48af-9675-b6057afc80c6",
   "metadata": {},
   "source": [
    "### Test split -- 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d791947-e38a-4ae1-bbe6-50414d91f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p5[\"splitData\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8224501-aea2-49ea-9951-e45803d58582",
   "metadata": {},
   "source": [
    "### Test split -- 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3230fd-1296-4943-a071-c55a840c4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p5[\"splitData\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467984ed-d4aa-4143-91c8-1b17dbd9cf93",
   "metadata": {},
   "source": [
    "### Test split -- 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b5878-1d9e-4be6-bcdd-ce6025848a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p5[\"splitData\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cfba4-a6b7-403e-9e08-ea54bf9d7d9d",
   "metadata": {},
   "source": [
    "### Test split -- 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77904dd-d126-4929-acd4-aacf83931ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressor(p5[\"splitData\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e43d1-0770-4945-9295-c505ed3a3cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
